# Noise Augmentation + Training (AlexNet / DenseNet-121)

**Goal:** Recreate the *noise* experiments from the paper.  
This notebook:
- Loads a prepared base dataset (`png_OR` recommended).
- Adds **Gaussian**, **Salt & Pepper**, and **Poisson** noise (each kept as a variant) and keeps the original.
- (Optional) tiles into **4x** or **16x** to match `_4x` / `_16x`.
- Trains **from scratch** with **AlexNet** or **DenseNet-121**.
- Saves **history.csv**, **predict.csv**, **y_true.csv**, **labels.txt`.

## How to use (Colab)
1. Set in “User Config”:
   - `SOURCE_DIR`: e.g., `/content/drive/MyDrive/microlab-data/prepared/png_OR`
   - `AUG_NAME`: e.g., `N_16x`
   - `DO_TILE` / `TILE_FACTOR`: `4` or `16`
   - `MODEL_NAME`: `"alexnet"` or `"densenet121"`
2. **Run all**. Outputs appear in `microlab-outputs/<model>_noise_<AUG_NAME>/`.

## Notes
- **No pretrained weights** (training from scratch).
- Only **accuracy** and **loss** plots are produced.
- Keep the class-per-folder structure.

# ================================================================
#  Noise Augmentation + Train (AlexNet or DenseNet-121)
#  --------------------------------------------------------------
#  Adds Gaussian, Salt&Pepper, and Poisson noise to each image
#  and keeps the original. Optional tiling to 4x/16x.
#
#  Outputs: history.csv, predict.csv, y_true.csv, labels.txt
# ================================================================

!nvidia-smi -L || true

import os, sys, math, time, random, shutil
from pathlib import Path

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from PIL import Image
from skimage.util import random_noise
import matplotlib.pyplot as plt

SEED = 123
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

USE_DRIVE = True
if USE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')

# =======================
# User Config (edit)
# =======================
BASE_DATA   = '/content/drive/MyDrive/microlab-data/prepared' if USE_DRIVE else '/content/microlab-data/prepared'
SOURCE_DIR  = f'{BASE_DATA}/png_OR'   # base folder for clean images
AUG_NAME    = 'N_16x'                 # e.g., N_4x or N_16x
DO_TILE     = True
TILE_FACTOR = 16                      # 4 or 16

MODEL_NAME  = 'densenet121'           # 'alexnet' or 'densenet121'
IMG_SIZE    = (224, 224) if MODEL_NAME=='densenet121' else (227, 227)
BATCH_SIZE  = 64
EPOCHS      = 25
LR          = 0.001 if MODEL_NAME=='densenet121' else 0.0015

OUT_ROOT    = '/content/drive/MyDrive/microlab-outputs' if USE_DRIVE else '/content/outputs'
OUT_DIR     = f'{OUT_ROOT}/{MODEL_NAME}_noise_{AUG_NAME}'
os.makedirs(OUT_DIR, exist_ok=True)

print("SOURCE_DIR:", SOURCE_DIR)
print("AUG_NAME  :", AUG_NAME)
print("MODEL     :", MODEL_NAME)
print("OUT_DIR   :", OUT_DIR)

def ensure_clean_dir(path):
    p = Path(path)
    if p.exists(): shutil.rmtree(p)
    p.mkdir(parents=True, exist_ok=True)

def add_gaussian(np_img, var=0.01):
    return random_noise(np_img, mode='gaussian', var=var, clip=True)

def add_sp(np_img, amount=0.02):
    return random_noise(np_img, mode='s&p', amount=amount, clip=True)

def add_poisson(np_img):
    return random_noise(np_img, mode='poisson', clip=True)

def save_augmented_set(src_dir, dst_dir, do_tile=False, tile_factor=4):
    src = Path(src_dir); dst = Path(dst_dir)
    ensure_clean_dir(dst)

    for cls_dir in sorted(p for p in src.iterdir() if p.is_dir()):
        (dst/cls_dir.name).mkdir(parents=True, exist_ok=True)
        print(f"[Augment|noise] Class:", cls_dir.name)

        for img_path in sorted(cls_dir.glob('*.jpg')) + sorted(cls_dir.glob('*.png')):
            try:
                img = Image.open(img_path).convert('RGB')
            except Exception as e:
                print("Skip:", img_path, e); continue

            np_img = np.asarray(img) / 255.0
            variants = [np_img,
                        add_gaussian(np_img, var=0.01),
                        add_sp(np_img, amount=0.02),
                        add_poisson(np_img)]

            base = img_path.stem
            for idx, arr in enumerate(variants):
                arr_u8 = (np.clip(arr, 0, 1) * 255).astype(np.uint8)
                im = Image.fromarray(arr_u8)

                if do_tile:
                    w, h = im.size
                    tx = 2 if tile_factor==4 else 4
                    ty = 2 if tile_factor==4 else 4
                    tile_w = w // tx; tile_h = h // ty
                    for r in range(ty):
                        for c in range(tx):
                            box = (c*tile_w, r*tile_h, (c+1)*tile_w, (r+1)*tile_h)
                            im.crop(box).save(dst/cls_dir.name/f"{base}_n{idx}_{r}_{c}.jpg", quality=95)
                else:
                    im.save(dst/cls_dir.name/f"{base}_n{idx}.jpg", quality=95)

AUG_DIR = f"{BASE_DATA}/{AUG_NAME}"
print("Building augmented dataset at:", AUG_DIR)
save_augmented_set(SOURCE_DIR, AUG_DIR, DO_TILE, TILE_FACTOR)

# ---- Load dataset
train_ds = tf.keras.utils.image_dataset_from_directory(
    AUG_DIR, validation_split=0.2, subset='training', seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE
)
val_ds = tf.keras.utils.image_dataset_from_directory(
    AUG_DIR, validation_split=0.2, subset='validation', seed=SEED,
    image_size=IMG_SIZE, batch_size=BATCH_SIZE
)

class_names = train_ds.class_names; num_classes = len(class_names)
with open(os.path.join(OUT_DIR, 'labels.txt'),'w') as f:
    for c in class_names: f.write(c+'\n')
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(AUTOTUNE)
val_ds   = val_ds.cache().prefetch(AUTOTUNE)

# quick viz
plt.figure(figsize=(9,9))
for images, labels in train_ds.take(1):
    for i in range(min(9, images.shape[0])):
        ax = plt.subplot(3,3,i+1)
        plt.imshow(images[i].numpy().astype('uint8'))
        plt.title(class_names[int(labels[i])], fontsize=8); plt.axis('off')
plt.tight_layout(); plt.show()

# ---- Models
def build_alexnet(input_shape=(227,227,3), num_classes=33, lr=0.0015):
    m = keras.Sequential([
        layers.Conv2D(96,(11,11),strides=(4,4),activation='relu',input_shape=input_shape),
        layers.BatchNormalization(),
        layers.MaxPool2D((3,3),(2,2)),
        layers.Conv2D(256,(5,5),padding='same',activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPool2D((3,3),(2,2)),
        layers.Conv2D(384,(3,3),padding='same',activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(384,(1,1),padding='same',activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(256,(1,1),padding='same',activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPool2D((3,3),(2,2)),
        layers.Flatten(),
        layers.Dense(4096,activation='relu'), layers.Dropout(0.5),
        layers.Dense(4096,activation='relu'), layers.Dropout(0.5),
        layers.Dense(num_classes,activation='softmax')
    ])
    m.compile(optimizer=tf.keras.optimizers.SGD(LR, momentum=0.9),
              loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return m

def build_densenet121_from_scratch(input_shape=(224,224,3), num_classes=33, lr=0.001):
    base = tf.keras.applications.DenseNet121(include_top=False, weights=None, input_shape=input_shape)
    inp = keras.Input(shape=input_shape)
    x = base(inp, training=True)
    x = layers.GlobalAveragePooling2D()(x)
    out= layers.Dense(num_classes, activation='softmax')(x)
    m = keras.Model(inp, out)
    m.compile(optimizer=tf.keras.optimizers.SGD(lr, momentum=0.9),
              loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return m

model = build_alexnet((IMG_SIZE[0],IMG_SIZE[1],3), num_classes, LR) if MODEL_NAME=='alexnet' \
        else build_densenet121_from_scratch((IMG_SIZE[0],IMG_SIZE[1],3), num_classes, LR)
model.summary()

# ---- Train
history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)

# ---- Save outputs
pred_probs = model.predict(val_ds, verbose=1)
y_true = np.concatenate([y for _, y in val_ds.as_numpy_iterator()], axis=0)
np.savetxt(os.path.join(OUT_DIR, 'predict.csv'), pred_probs, delimiter=',')
np.savetxt(os.path.join(OUT_DIR, 'y_true.csv'), y_true, delimiter=',')

import pandas as pd
pd.DataFrame.from_dict(history.history).to_csv(os.path.join(OUT_DIR, 'history.csv'), index=False)

# ---- Plots
plt.figure(figsize=(7,5))
plt.plot(history.history['accuracy']); plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy'); plt.ylabel('Accuracy'); plt.xlabel('Epoch'); plt.legend(['Train','Validation'])
plt.grid(alpha=.3); plt.tight_layout(); plt.show()

plt.figure(figsize=(7,5))
plt.plot(history.history['loss']); plt.plot(history.history['val_loss'])
plt.title('Model Loss'); plt.ylabel('Loss'); plt.xlabel('Epoch'); plt.legend(['Train','Validation'])
plt.grid(alpha=.3); plt.tight_layout(); plt.show()
