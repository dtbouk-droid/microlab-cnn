# ================================================================
#  AlexNet Training (from scratch) on prepared DIBaS datasets
#  --------------------------------------------------------------
#  Expects directory structure:
#     DATA_DIR/
#        class_1/*.jpg|*.png
#        class_2/*.jpg|*.png
#        ...
#  Works with any of your prepared sets (e.g., jpg_OR, jpg_4x, jpg_16x,
#  png_OR, png_4x, png_16x, or augmented folders).
#
#  Outputs:
#   - history.csv           (training log: loss/acc)
#   - predict.csv           (validation probabilities, shape: [N, num_classes])
#   - y_true.csv            (validation ground-truth labels, length N)
#   - labels.txt            (class names, one per line)
#
#  Author: YOUR NAME
#  License: MIT
# ================================================================

# --- (Optional) see GPU info
!nvidia-smi -L || true

# --- Imports
import os, sys, math, time, random, json
from pathlib import Path

import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

# Reproducibility
SEED = 123
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# --- (Optional) Mount Drive to save outputs persistently
USE_DRIVE = True  # set False if you prefer /content only
if USE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')

# ===========================
# User Config (edit here)
# ===========================
# 1) Select which prepared dataset to use
BASE_DATA = '/content/drive/MyDrive/microlab-data/prepared' if USE_DRIVE else '/content/microlab-data/prepared'
DATA_DIR   = f'{BASE_DATA}/jpg_16x'       # e.g., jpg_OR, jpg_4x, jpg_16x, png_OR, png_4x, png_16x, augmented/M_16x, etc.

# 2) Training hyperparameters
IMG_SIZE   = (227, 227)  # AlexNet input
BATCH_SIZE = 128         # 64 for OR; 128 works well for 4x/16x (per your study)
EPOCHS     = 50
LR         = 0.0015
MOMENTUM   = 0.9

# 3) Output directory for CSVs/plots
OUT_DIR    = '/content/outputs_alexnet' if not USE_DRIVE else '/content/drive/MyDrive/microlab-outputs/alexnet'
os.makedirs(OUT_DIR, exist_ok=True)

print("DATA_DIR:", DATA_DIR)
print("OUT_DIR :", OUT_DIR)

# ===========================
# Dataset loading (80/20)
# ===========================
data_path = Path(DATA_DIR)
assert data_path.exists() and any(data_path.glob('*/*')), \
    f"DATA_DIR '{DATA_DIR}' must have subfolders per class with images."

train_ds = tf.keras.utils.image_dataset_from_directory(
    data_path,
    validation_split=0.2,
    subset='training',
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    crop_to_aspect_ratio=False
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_path,
    validation_split=0.2,
    subset='validation',
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    crop_to_aspect_ratio=False
)

class_names = train_ds.class_names
num_classes = len(class_names)
print("Detected classes:", num_classes)
print(class_names[:10], "..." if len(class_names) > 10 else "")

# Save labels
with open(os.path.join(OUT_DIR, 'labels.txt'), 'w') as f:
    for c in class_names:
        f.write(c + '\n')

# Configure for performance
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# ===========================
# Visual sanity check (1 batch)
# ===========================
plt.figure(figsize=(9, 9))
for images, labels in train_ds.take(1):
    for i in range(min(9, images.shape[0])):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype('uint8'))
        plt.title(class_names[int(labels[i].numpy())], fontsize=8)
        plt.axis('off')
plt.suptitle('Sample training images', y=1.02)
plt.tight_layout()
plt.show()

# ===========================
# AlexNet Model (Keras)
# ===========================
def build_alexnet(input_shape=(227,227,3), num_classes=33):
    model = keras.models.Sequential([
        # Conv1
        keras.layers.Conv2D(96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
        # Conv2
        keras.layers.Conv2D(256, kernel_size=(5,5), strides=(1,1), activation='relu', padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
        # Conv3-5
        keras.layers.Conv2D(384, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(384, kernel_size=(1,1), strides=(1,1), activation='relu', padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(256, kernel_size=(1,1), strides=(1,1), activation='relu', padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
        keras.layers.Flatten(),
        # FC
        keras.layers.Dense(4096, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(4096, activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(num_classes, activation='softmax')
    ])
    return model

model = build_alexnet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=num_classes)

# Original paper training setup (adapted to your best LR):
optimizer = tf.keras.optimizers.SGD(learning_rate=LR, momentum=MOMENTUM, nesterov=False)
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=optimizer,
    metrics=['accuracy']
)

model.summary()

# ===========================
# Training
# ===========================
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    verbose=1
)

# ===========================
# Evaluation on train/val
# ===========================
print("\n— Evaluate on TRAIN —")
train_loss, train_acc = model.evaluate(train_ds, verbose=0)
print(f"Train:  loss={train_loss:.4f}  acc={train_acc:.4f}")

print("\n— Evaluate on VALIDATION —")
val_loss, val_acc = model.evaluate(val_ds, verbose=0)
print(f"Val:    loss={val_loss:.4f}  acc={val_acc:.4f}")

# ===========================
# Predictions & y_true saving
# ===========================
# Predictions on the entire validation set
print("\nGenerating predictions for validation set...")
pred_probs = model.predict(val_ds, verbose=1)  # shape: [N, num_classes]

# Collect y_true from val_ds
y_true_list = []
for _, y in val_ds.as_numpy_iterator():
    y_true_list.append(y)
y_true = np.concatenate(y_true_list, axis=0)  # shape: [N,]

# Save CSVs
np.savetxt(os.path.join(OUT_DIR, 'predict.csv'), pred_probs, delimiter=',')
np.savetxt(os.path.join(OUT_DIR, 'y_true.csv'), y_true, delimiter=',')

# Save training history
import pandas as pd
pd.DataFrame.from_dict(history.history).to_csv(os.path.join(OUT_DIR, 'history.csv'), index=False)

print("Saved:")
print(" -", os.path.join(OUT_DIR, 'predict.csv'))
print(" -", os.path.join(OUT_DIR, 'y_true.csv'))
print(" -", os.path.join(OUT_DIR, 'history.csv'))
print(" -", os.path.join(OUT_DIR, 'labels.txt'))

# ===========================
# Plots: Accuracy & Loss
# ===========================
plt.figure(figsize=(7,5))
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

plt.figure(figsize=(7,5))
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
