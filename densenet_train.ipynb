# ================================================================
#  DenseNet-121 Training (from scratch) on prepared DIBaS datasets
#  --------------------------------------------------------------
#  Expects directory structure:
#     DATA_DIR/
#        class_1/*.jpg|*.png
#        class_2/*.jpg|*.png
#        ...
#  Works with any of your prepared sets (e.g., jpg_OR, jpg_4x, jpg_16x,
#  png_OR, png_4x, png_16x, or augmented folders).
#
#  Outputs:
#   - history.csv           (training log: loss/acc)
#   - predict.csv           (validation probabilities, shape: [N, num_classes])
#   - y_true.csv            (validation ground-truth labels, length N)
#   - labels.txt            (class names, one per line)
#
#  Notes (per your paper):
#   - Train FROM SCRATCH (weights=None). No transfer learning.
#   - IMG_SIZE = (224, 224)
#   - Batch size often capped at 64 by RAM.
#   - LR ~ 0.001 commonly best; Epochs ~ 25 for cropped sets, ~50 for OR.
#
#  Author: YOUR NAME
#  License: MIT
# ================================================================

# --- (Optional) see GPU info
!nvidia-smi -L || true

# --- Imports
import os, sys, math, time, random, json
from pathlib import Path

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# Reproducibility
SEED = 123
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# --- (Optional) Mount Drive to save outputs persistently
USE_DRIVE = True  # set False if you prefer /content only
if USE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')

# ===========================
# User Config (edit here)
# ===========================
# 1) Select which prepared dataset to use
BASE_DATA = '/content/drive/MyDrive/microlab-data/prepared' if USE_DRIVE else '/content/microlab-data/prepared'
DATA_DIR   = f'{BASE_DATA}/jpg_4x'       # e.g., jpg_OR, jpg_4x, jpg_16x, png_OR, png_4x, png_16x, augmented/A_16x, etc.

# 2) Training hyperparameters
IMG_SIZE   = (224, 224)  # DenseNet-121 input
BATCH_SIZE = 64          # cap at 64 to avoid OOM (as in your study)
# Recommended: 25 epochs for cropped (_4x/_16x), ~50 for _OR
EPOCHS     = 25
LR         = 0.001

# 3) Output directory for CSVs/plots
OUT_DIR    = '/content/outputs_densenet' if not USE_DRIVE else '/content/drive/MyDrive/microlab-outputs/densenet'
os.makedirs(OUT_DIR, exist_ok=True)

print("DATA_DIR:", DATA_DIR)
print("OUT_DIR :", OUT_DIR)

# ===========================
# Dataset loading (80/20)
# ===========================
data_path = Path(DATA_DIR)
assert data_path.exists() and any(data_path.glob('*/*')), \
    f"DATA_DIR '{DATA_DIR}' must have subfolders per class with images."

train_ds = tf.keras.utils.image_dataset_from_directory(
    data_path,
    validation_split=0.2,
    subset='training',
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    crop_to_aspect_ratio=False
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_path,
    validation_split=0.2,
    subset='validation',
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    crop_to_aspect_ratio=False
)

class_names = train_ds.class_names
num_classes = len(class_names)
print("Detected classes:", num_classes)
print(class_names[:10], "..." if len(class_names) > 10 else "")

# Save labels
with open(os.path.join(OUT_DIR, 'labels.txt'), 'w') as f:
    for c in class_names:
        f.write(c + '\n')

# Configure for performance
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# ===========================
# Visual sanity check (1 batch)
# ===========================
plt.figure(figsize=(9, 9))
for images, labels in train_ds.take(1):
    for i in range(min(9, images.shape[0])):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype('uint8'))
        plt.title(class_names[int(labels[i].numpy())], fontsize=8)
        plt.axis('off')
plt.suptitle('Sample training images', y=1.02)
plt.tight_layout()
plt.show()

# ===========================
# DenseNet-121 (from scratch)
# ===========================
def build_densenet121_from_scratch(input_shape=(224,224,3), num_classes=33, lr=0.001):
    # Use Keras Applications backbone with weights=None (from scratch)
    base = tf.keras.applications.DenseNet121(
        include_top=False,
        weights=None,
        input_shape=input_shape
    )
    inputs = keras.Input(shape=input_shape)
    x = inputs
    # Optional: normalization to [0, 1] already; you may also center/scale if desired
    x = base(x, training=True)
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs, name='DenseNet121_scratch')

    optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)  # SGD works well here too
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

model = build_densenet121_from_scratch(
    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),
    num_classes=num_classes,
    lr=LR
)

model.summary()

# ===========================
# Training
# ===========================
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    verbose=1
)

# ===========================
# Evaluation on train/val
# ===========================
print("\n— Evaluate on TRAIN —")
train_loss, train_acc = model.evaluate(train_ds, verbose=0)
print(f"Train:  loss={train_loss:.4f}  acc={train_acc:.4f}")

print("\n— Evaluate on VALIDATION —")
val_loss, val_acc = model.evaluate(val_ds, verbose=0)
print(f"Val:    loss={val_loss:.4f}  acc={val_acc:.4f}")

# ===========================
# Predictions & y_true saving
# ===========================
print("\nGenerating predictions for validation set...")
pred_probs = model.predict(val_ds, verbose=1)  # shape: [N, num_classes]

# Collect y_true from val_ds
y_true_list = []
for _, y in val_ds.as_numpy_iterator():
    y_true_list.append(y)
y_true = np.concatenate(y_true_list, axis=0)  # shape: [N,]

# Save CSVs
np.savetxt(os.path.join(OUT_DIR, 'predict.csv'), pred_probs, delimiter=',')
np.savetxt(os.path.join(OUT_DIR, 'y_true.csv'), y_true, delimiter=',')

# Save training history
import pandas as pd
pd.DataFrame.from_dict(history.history).to_csv(os.path.join(OUT_DIR, 'history.csv'), index=False)

print("Saved:")
print(" -", os.path.join(OUT_DIR, 'predict.csv'))
print(" -", os.path.join(OUT_DIR, 'y_true.csv'))
print(" -", os.path.join(OUT_DIR, 'history.csv'))
print(" -", os.path.join(OUT_DIR, 'labels.txt'))

# ===========================
# Plots: Accuracy & Loss
# ===========================
plt.figure(figsize=(7,5))
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

plt.figure(figsize=(7,5))
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

